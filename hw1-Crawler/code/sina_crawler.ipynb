{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 大作业part1：python爬虫\n",
    "> 张驰 zhang-ch22@mails.tsinghua.edu.cn Isomorphic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wdw\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import selenium\n",
    "from time import sleep\n",
    "options = selenium.webdriver.ChromeOptions()\n",
    "options.add_argument('User-Agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0')\n",
    "prefs = {\n",
    "    'profile.default_content_setting_values':{\n",
    "        'images':2,\n",
    "    }\n",
    "}\n",
    "options.add_experimental_option(\"excludeSwitches\", ['enable-automation'])\n",
    "# options.add_experimental_option('prefs',prefs)\n",
    "# options.add_argument(\"--disable-plugins\")\n",
    "driver = selenium.webdriver.Chrome(options = options)\n",
    "print(\"Done\")"
   ]
  },
  {
   "source": [
    "首先爬取首页，获得新闻正文页链接，存放到links.csv中"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "with open(\"links.csv\",'w',encoding='utf-8') as file:\n",
    "    file.write(\"title,link\\n\")\n",
    "    for page in range(1,51):\n",
    "        print(\"current page:\",page)\n",
    "        driver.get(f\"https://news.sina.com.cn/roll/#pageid=153&lid=2515&k=&num=50&page={page}\")\n",
    "        driver.refresh()\n",
    "        driver.execute_script(\"window.scrollBy(0, 1);\")\n",
    "        spans = driver.find_elements(By.CLASS_NAME,'c_tit')\n",
    "        count = 0\n",
    "        for span in spans:\n",
    "            count+=1\n",
    "            try:\n",
    "                site = span.find_element(By.TAG_NAME,'a')\n",
    "                link = site.get_attribute('href')\n",
    "                title = site.text\n",
    "                file.write(title+','+link+'\\n')\n",
    "            except StaleElementReferenceException:\n",
    "                print(\"Exception:\",page,count)\n",
    "                continue\n",
    "\n",
    "print(\"Done\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "这些为2023.8.28的新闻，不够5000条。下面爬取过去一年的新闻。2022.8.28-2023.8.27\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import *\n",
    "#2022-08-28\n",
    "etime0 = 1670428800\n",
    "stime0 = 1670515200\n",
    "date0 = datetime(2022,8,28).date()\n",
    "\n",
    "with open(\"links1year.csv\",'a',encoding='utf-8') as file:\n",
    "\n",
    "    for days in range(-1396,-10000,-1):\n",
    "\n",
    "        if days%100==0:\n",
    "            driver.close()\n",
    "            options = selenium.webdriver.ChromeOptions()\n",
    "            options.add_argument('User-Agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0')\n",
    "           \n",
    "            options.add_experimental_option(\"excludeSwitches\", ['enable-automation'])\n",
    "\n",
    "            driver = selenium.webdriver.Chrome(options = options)\n",
    "\n",
    "        etime = etime0 + 86400*days\n",
    "        stime = stime0 + 86400*days\n",
    "        date = date0 + timedelta(days=days)\n",
    "        datestr = date.strftime('%Y-%m-%d')\n",
    "        print(\"current date:\",datestr)\n",
    "        page=0\n",
    "        while True:\n",
    "            page+=1\n",
    "            url = f\"https://news.sina.com.cn/roll/#pageid=153&lid=2515&etime={etime}&stime={stime}&ctime={stime}&date={datestr}&k=&num=50&page={page}\"\n",
    "            \n",
    "            driver.get(url)\n",
    "            driver.refresh()\n",
    "            driver.execute_script(\"window.scrollBy(0, 1);\")\n",
    "            spans = driver.find_elements(By.CLASS_NAME,'c_tit')\n",
    "            count = 0\n",
    "            for span in spans:\n",
    "                count+=1\n",
    "                try:\n",
    "                    site = span.find_element(By.TAG_NAME,'a')\n",
    "                    link = site.get_attribute('href')\n",
    "                    title = site.text\n",
    "                    file.write(title+','+link+'\\n')\n",
    "                except:\n",
    "                    print(\"Exception:\",page,count)\n",
    "                    continue\n",
    "            if count<50:break\n",
    "\n",
    "print(\"Done\")\n",
    "with open(\"links1year.csv\",'r',encoding='utf-8') as infile, open(\"links_processed.csv\",'w',encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        index = line.rfind(\",\")\n",
    "        outline = f'\"{line[:index]}\",\"{line[index+1:]}\"'\n",
    "        outfile.write(outline+'\\n')"
   ]
  },
  {
   "source": [
    "储存的csv文件有的行自带“,”导致分块出现问题，下统一处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "之后再读取links_processed.csv中的网址，爬取新闻正文\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def extract_text(p_elem):\n",
    "    try:\n",
    "        child_elem = p_elem.find_element(By.CSS_SELECTOR,'*')\n",
    "        return extract_text(child_elem)\n",
    "    except NoSuchElementException:\n",
    "        return p_elem.text\n",
    "        \n",
    "# df = pd.read_csv(\"links.csv\",encoding='utf-8')\n",
    "df = pd.read_csv(\"links_processed.csv\",encoding='utf-8')\n",
    "links = df[\"link\"].values #python list\n",
    "titles = df[\"title\"].values\n",
    "N = df.shape[0]\n",
    "for i in range(220,N):# 此处从249开始\n",
    "\n",
    "    if i%100==0:\n",
    "        driver.close()\n",
    "        options = selenium.webdriver.ChromeOptions()\n",
    "        options.add_argument('User-Agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0')\n",
    "        prefs = {\n",
    "            'profile.default_content_setting_values':{\n",
    "                'images':2,\n",
    "                'css':2\n",
    "            }\n",
    "        }\n",
    "        options.add_experimental_option(\"excludeSwitches\", ['enable-automation'])\n",
    "        # options.add_experimental_option('prefs',prefs)\n",
    "        # options.add_argument(\"--disable-plugins\")\n",
    "        driver = selenium.webdriver.Chrome(options = options)\n",
    "    print(i)\n",
    "    link = links[i]\n",
    "    driver.get(link)\n",
    "    driver.refresh()\n",
    "\n",
    "    author = \"NaN\"\n",
    "    publish_time = \"NaN\"\n",
    "    try:\n",
    "        publish_time = extract_text(driver.find_element(By.CLASS_NAME,'date'))\n",
    "    except NoSuchElementException:\n",
    "        try: publish_time = extract_text(driver.find_element(By.ID,'pub_date'))\n",
    "        except: pass\n",
    "    try:\n",
    "        author = extract_text(driver.find_element(By.CLASS_NAME,'source'))\n",
    "    except NoSuchElementException:\n",
    "        try: \n",
    "            author = extract_text(driver.find_element(By.CLASS_NAME,'author'))\n",
    "        except NoSuchElementException: \n",
    "            try: author = author = extract_text(driver.find_element(By.ID,'media_name'))\n",
    "            except: pass\n",
    "\n",
    "    article = []\n",
    "    try:\n",
    "        artibody = driver.find_element(By.ID,'artibody')\n",
    "        paras = artibody.find_elements(By.CSS_SELECTOR,'*')\n",
    "        for para in paras:\n",
    "            if para.tag_name=='p':\n",
    "                article.append(para.text)\n",
    "            else:\n",
    "                try:\n",
    "                    img = para.find_element(By.TAG_NAME,'img')\n",
    "                    img_link = img.get_attribute(\"src\")\n",
    "                    img_text = img.get_attribute(\"alt\")\n",
    "                    article.append([img_link,img_text])\n",
    "                except: pass\n",
    "    except: pass\n",
    "\n",
    "    comments = []\n",
    "    try: \n",
    "        comment_page = driver.find_element(By.CLASS_NAME,'sina-comment-page')\n",
    "        heads = comment_page.find_elements(By.CLASS_NAME,'head')\n",
    "        contents = comment_page.find_elements(By.CLASS_NAME,'cont')\n",
    "        for c,h in zip(contents,heads):\n",
    "            area = \"NaN\"\n",
    "            try: area = c.find_element(By.CLASS_NAME,'area').text\n",
    "            except: pass\n",
    "            text = \"NaN\"\n",
    "            try: text = c.find_element(By.CLASS_NAME,'txt').text\n",
    "            except: pass\n",
    "            head_img = \"NaN\"\n",
    "            try: head_img = h.find_element(By.TAG_NAME,'img').get_attribute('src')\n",
    "            except: pass\n",
    "            time = \"NaN\"\n",
    "            try: time = c.find_element(By.CLASS_NAME,'time').text\n",
    "            except: pass\n",
    "            name = \"NaN\"\n",
    "            try: name = h.find_element(By.TAG_NAME,'a').get_attribute('title')\n",
    "            except: pass\n",
    "            comments.append([name,head_img,area,text,time])\n",
    "    except: pass\n",
    "\n",
    "    comment_count = 'NaN'\n",
    "    comment_users = 'NaN' #参与用户数\n",
    "    try: \n",
    "        count = driver.find_element(By.CLASS_NAME,'count')\n",
    "        twoelems = count.find_elements(By.TAG_NAME,'a')\n",
    "        print(len(twoelems))\n",
    "        if len(twoelems)==2:\n",
    "            comment_count = twoelems[0].text\n",
    "            comment_users = twoelems[1].text\n",
    "    except: pass\n",
    "\n",
    "    title = titles[i]\n",
    "    link = links[i]\n",
    "    data = {\"title\":title,\"pub_date\":publish_time,\"author\":author,\"article\":article,\"comment_count\":comment_count,\"comment_users\":comment_users,\"comments\":comments,\"link\":link}\n",
    "    df = pd.DataFrame(data.items())\n",
    "    df.to_csv(f\"./results/{i+1}.csv\",index=False,encoding='utf-8')\n",
    "    "
   ]
  },
  {
   "source": [
    "爬取的结果都在\"./results\"里，每个网页有一个独立的.csv文件\n",
    "\n",
    "由于selenium太慢，下面不用selenium重写了一遍"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from time import sleep\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "def extract_text(p_elem):\n",
    "    if p_elem.find() is not None:\n",
    "        return extract_text(p_elem.find())\n",
    "    else:\n",
    "        return p_elem.text\n",
    "\n",
    "df = pd.read_csv(\"links_processed.csv\",encoding='utf-8')\n",
    "links = df[\"link\"].values #python list\n",
    "titles = df[\"title\"].values\n",
    "N = df.shape[0]\n",
    "\n",
    "for i in range(N):# 此处从249开始\n",
    "    if i%11==5: sleep(1)\n",
    "    if i%37==6: sleep(2)\n",
    "    if i%97==7: sleep(5)\n",
    "\n",
    "    print(i)\n",
    "    link = links[i]\n",
    "    ua=UserAgent()\n",
    "    headers={\"User-Agent\":ua.random} \n",
    "    response = requests.get(link,headers=headers)\n",
    "    sleep(0.1)\n",
    "    response.encoding = 'UTF-8'\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "    author = \"\"\n",
    "\n",
    "    a = soup.find(class_='source')\n",
    "    if a is not None:\n",
    "        author = extract_text(a)\n",
    "    else:\n",
    "        b = soup.find(class_='author')\n",
    "        if b is not None:\n",
    "            author = extract_text(b)\n",
    "        else:\n",
    "            c = soup.find(id='media_name')\n",
    "            if c is not None:\n",
    "                author = extract_text(c)\n",
    "\n",
    "    article = []\n",
    "    artibody = soup.find(id='artibody')\n",
    "    if artibody is not None:\n",
    "        paras = artibody.find_all()\n",
    "        for para in paras:\n",
    "            if para.name=='p':\n",
    "                article.append(para.text)\n",
    "            elif 'class' in para.attrs:\n",
    "                if para.attrs['class']==['img_wrapper']:\n",
    "                    img = para.find('img')\n",
    "                    \n",
    "                    if img is not None:\n",
    "                        img_link = \"\"\n",
    "                        img_text = \"\"\n",
    "                        if 'src' in img.attrs:\n",
    "                            img_link = \"https:\"+img.attrs[\"src\"]\n",
    "                        if 'alt' in img.attrs:\n",
    "                            img_text = img.attrs[\"alt\"]\n",
    "                        article.append([img_link,img_text])\n",
    "\n",
    "    match = re.search(r\"doc-i(.+?)\\.shtml\",link)\n",
    "    newsid = match.group(1)\n",
    "    comment_url_kj = f\"https://comment5.news.sina.com.cn/page/info?version=1&format=json&channel=kj&newsid=comos-{newsid}&group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1\"\n",
    "    comment_url_cj = f\"https://comment5.news.sina.com.cn/page/info?version=1&format=json&channel=cj&newsid=comos-{newsid}&group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1\"\n",
    "\n",
    "    comment_count_cj = 0\n",
    "    comment_count_kj = 0\n",
    "    comment_users_cj = 0\n",
    "    comment_users_kj = 0\n",
    "\n",
    "    response_cj = requests.get(comment_url_cj)\n",
    "    response_kj = requests.get(comment_url_kj)\n",
    "    sleep(0.1)\n",
    "\n",
    "    response_cj.encoding = 'UTF-8'\n",
    "    cmtpg_cj = response_cj.text\n",
    "    jd_cj = json.loads(cmtpg_cj)\n",
    "    if 'count' in jd_cj['result']:\n",
    "        cntcj = jd_cj['result']['count']\n",
    "        comment_count_cj = cntcj['show']\n",
    "        comment_users_cj = cntcj['total']\n",
    "\n",
    "    response_kj.encoding = 'UTF-8'\n",
    "    cmtpg_kj = response_kj.text\n",
    "    jd_kj = json.loads(cmtpg_kj)\n",
    "    if 'count' in jd_kj['result']:\n",
    "        cntkj = jd_kj['result']['count']\n",
    "        comment_count_kj = cntkj['show']\n",
    "        comment_users_kj = cntkj['total'] \n",
    "\n",
    "    comment_count = max(comment_count_cj,comment_count_kj)\n",
    "    comment_users = max(comment_users_cj,comment_users_kj)\n",
    "\n",
    "    pub_date = \"\"\n",
    "    if 'news' in jd_cj['result']:\n",
    "        pub_date = jd_cj['result']['news']['time']\n",
    "    if pub_date=='' and 'news' in jd_kj['result']:\n",
    "        pub_date = jd_kj['result']['news']['time']\n",
    "\n",
    "    title = titles[i]\n",
    "    link = links[i]\n",
    "    data = {\"title\":title,\"pub_date\":pub_date,\"author\":author,\"article\":article,\"comment_count\":comment_count,\"comment_users\":comment_users,\"link\":link}\n",
    "    df = pd.DataFrame(data.items())\n",
    "    df.to_csv(f\"./results/{i+1}.csv\",index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}